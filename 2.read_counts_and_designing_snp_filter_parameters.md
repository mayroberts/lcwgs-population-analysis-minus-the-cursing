# Getting there...I think?
- [Calculate coverage and determine appropriate snp calling parameters](#Calculate-final-coverage)
- [Summarize the depth info](#Summarize-the-depth-info)

Calculate read counts to check what you're working with and help design your snp filters 

## Calculate final coverage
This uses `samtools depth` and runs an array to count depth per position for each .bam file. The out put is a `bam.depth.gz` in the same place where your .bam files live. \
Run time: 20 min \
`1-count_depth_per_position_per_sample.mpi`

	#!/bin/bash
	#
	#SBATCH --job-name 1readdepth
	#SBATCH --output=%A_%a-1readdepth.out
	#SBATCH --error=%A_%a-1readdepth.err
	#SBATCH --mail-type=ALL
	#SBATCH --mail-user=mabrober@ucsc.edu
	#SBATCH --partition=128x24
	##SBATCH --nodes=1
	#SBATCH --time=7-00:00:00
	#SBATCH --mem=30GB
	##SBATCH --ntasks-per-node=6
	#SBATCH --array=1-132%12
	## This script is used to count per position depth for bam files. It will create one depth file per bam file.
	# Advantage of this version is that you can use R to process the depth files.

	module load samtools

	BAMLIST=/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists/bam_path_list.txt #$1 # Path to a list of merged bam files. Full paths should be included
	MINBASEQ=20 # Minimum base quality filter
	MINMAPQ=20 # Minimum mapping quality filter

	SAMPLEBAM=$(sed -n "${SLURM_ARRAY_TASK_ID},1p" $BAMLIST)
	## Count per position depth per sample
	samtools depth -aa $SAMPLEBAM -q $MINBASEQ -Q $MINMAPQ | cut -f 3 |gzip > ${SAMPLEBAM}.depth.gz
	
## Summarize the depth info 
For this we use R which means we need 2 scripts. 1, is the slurm job sumbission and the second is the actual R script. We also need to load all the necessary packages into the tidyverse conda environment. so just activate tidyverse and load the condas for `r-r.utils` `r-data.table` `r cowplot` etc.(? - hope I got them all in there - check this at some point) \

`2-summmarize_depth_per_position.mpi` 

	#!/bin/bash
	#
	#SBATCH --job-name 2sumreaddepth
	#SBATCH --output=%A_2sumdepth.out
	#SBATCH --error=%A_2sumdepth.err
	#SBATCH --mail-type=ALL
	#SBATCH --mail-user=mabrober@ucsc.edu
	#SBATCH --partition=128x24
	#SBATCH --nodes=1
	#SBATCH --time=9-00:00:00
	#SBATCH --mem=120GB
	#SBATCH --ntasks-per-node=20


	module load miniconda3.9
	conda activate tidyverse

	Rscript summarize_depth_per_position.r
	
The Rscript:\
`summmarize_depth_per_position.r`

	## This script is used to summarize all the individual bam.depth.gz files (output of samtools depth),
	# It will come up with summary statistics for each individual as well per position depth and presence/absence summed across all individuals

	args <- commandArgs(trailingOnly = TRUE)
	BAMLIST <- "/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists/bam_path_list.tsv" #args[1] # Path to a list of merged bam files. Full paths should be included. An example of such a bam list is /workdir/cod/greenland-cod/sample_lists/bam_list_merged.tsv
	SAMPLETABLE <- "/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists/sample_table_merged.tsv" # args[2] # Path to a sample table where the 1st column is the prefix of the MERGED bam files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The 5th column is population name and 6th column is the data type. An example of such a sample table is: /workdir/cod/greenland-cod/sample_lists/sample_table_merged.tsv
	BASEDIR <- "/hb/groups/bernardi_lab/may/DTR/population-analysis/bams/deduped_overlap_clipped_bams" # args[3] # Path to the base directory where adapter clipped fastq file are stored in a subdirectory titled "adapter_clipped" and into which output files will be written to separate subdirectories. An example for the Greenland cod data is: /workdir/cod/greenland-cod/

	library(tidyverse)
	library(data.table)
	bam_list <- read_tsv(BAMLIST, col_names = F)$X1
	bam_list_prefix <- str_extract(BAMLIST, "[^.]+")
	sample_table <- read_tsv(SAMPLETABLE)
	for (i in 1:length(bam_list)){
	  print(i)
	  print(pryr::mem_used())
	  depth <- fread(paste0(bam_list[i], ".depth.gz"))$V1
	  mean_depth <- mean(depth)
	  sd_depth <- sd(depth)
	  presence <- as.logical(depth)
	  proportion_of_reference_covered <- mean(presence)
	  if (i==1){
	    output <- data.frame(sample_seq_id=sample_table[i,1], mean_depth, sd_depth, proportion_of_reference_covered)
	    total_depth <- depth
	    total_presence <- presence
	  } else {
	    output <- rbind(output, cbind(sample_seq_id=sample_table[i,1], mean_depth, sd_depth, proportion_of_reference_covered))
	    total_depth <- total_depth + depth
	    total_presence <- total_presence + presence
	  }
	}
	write_tsv(output, paste0(bam_list_prefix, "_depth_per_position_per_sample_summary.tsv"))
	write_lines(total_depth, paste0(bam_list_prefix, "_depth_per_position_all_samples.txt"))
	write_lines(total_presence, paste0(bam_list_prefix, "_presence_per_position_all_samples.txt"))

	# Total Depth per Site across All Individuals (on server)
	total_depth <- fread(paste0(bam_list_prefix, "_depth_per_position_all_samples.txt"))
	total_presence <- fread(paste0(bam_list_prefix, "_presence_per_position_all_samples.txt"))
	total_depth_summary <- count(total_depth, by=V1)
	total_presence_summary <- count(total_presence, by=V1)
	
Now I have files (in the sample_lists directory lol):

	DTR_dedup_bam_paths_depth_per_position_all_samples_histogram.tsv
	DTR_dedup_bams_depth_per_position_all_samples.txt  (list of coverage per position)
	DTR_dedup_bams_depth_per_position_per_sample_summary.tsv (table of with row per sample and columns for mean depth, sd depth, and proportion of reference covered)
	DTR_dedup_bams_presence_per_position_all_samples.txt (list of number of samples present at each position)
	DTR_dedup_bams_presence_per_position_all_samples.tsv


Now we use `2.1-summary_stats_plot.mpi` to further summarized stats and plot a histogram (output of read_count.Rmd). We use the histogram which should be a normal distribution (though might have a long tail) to figure out what our min adn max depth should be for the next steps of snp calling and filtering.  

SO: for this part, I did a bunch of test runs of the "## Design filters for SNP calling " bit of the r script on the head node to adjust the min and max depth values. Apparently this part is kind of eyeballed. The aim is to chop off the ends of the `depth_hist` distribution to make it a normal distribution looking plot. `depth_hist` is depth on x axis and # of sites on the y-axis so it shows you a distribution of how many sites have 0 coverage (a lot), how many have 50x coverage, how many have 300x....etc. It ends you looking kind of like a normal distribution except for the front end and tail end - so we chop it off to make a nice normal distribution. Why? Haven't wrapped my head around that yet... 

 `2.1-summary_stats_plot.mpi` This calls on the R script 
 
	#!/bin/bash
	#
	#SBATCH --job-name sumstat
	#SBATCH --output=%A-sumstat.out
	#SBATCH --error=%A-sumstat.err
	#SBATCH --mail-type=ALL
	#SBATCH --mail-user=mabrober@ucsc.edu
	#SBATCH --partition=128x24
	##SBATCH --nodes=1
	#SBATCH --time=7-00:00:00
	#SBATCH --mem=100GB
	#SBATCH --cpus_per-task=1


	module load miniconda3.9
	conda activate tidyverse

	Rscript summary_stats.r
	
`summary_stats.r` The R script:

	#!/bin/bash
	#
	#SBATCH --job-name sumstat
	#SBATCH --output=%A-sumstat.out
	#SBATCH --error=%A-sumstat.err
	#SBATCH --mail-type=ALL
	#SBATCH --mail-user=mabrober@ucsc.edu
	#SBATCH --partition=128x24
	##SBATCH --nodes=1
	#SBATCH --time=7-00:00:00
	#SBATCH --mem=100GB
	#SBATCH --cpus_per-task=1


	module load miniconda3.9
	conda activate tidyverse

	Rscript summary_stats.r
	(tidyverse) [mabrober@hb fresh]$ cat summary_stats.r
	## Summary statistics
	library(tidyverse)
	library(cowplot)
	library(knitr)

	basedir <- '/hb/groups/bernardi_lab/may/DTR/population-analysis'

	# Read in the data

	per_position_per_ind <- read_tsv(paste0(basedir, "/sample_lists/dedup_bam_paths_depth_per_position_per_sample_summary.tsv"))
	depth_hist <- read_tsv(paste0(basedir,"/sample_lists/dedup_bam_paths_depth_per_position_all_samples_histogram.tsv"))
	presence_hist <- read_tsv(paste0(basedir,"/sample_lists/dedup_bam_paths_presence_per_position_all_samples_histogram.tsv"))

	# mean depth per position across all individuals
	mean_depth_per_position_all <- round(sum(depth_hist$by*depth_hist$n)/sum(depth_hist$n),2)

	# standard deviation
	sd <- sqrt(sum((depth_hist$by-254.58)^2*depth_hist$n)/sum(depth_hist$n))

	# total sites covered at least once
	sites_cov <- sum(filter(depth_hist, by>0)$n)
	# % of the reference genome
	percent_cov <- round((sites_cov)/sum(depth_hist$n)*100,2)

	# Put this in writing - find this in the slurm .out file
	cat("The mean depth per position across all individuals is", mean_depth_per_position_all, "and the standard deviation is", sd)
	cat(". A total of", sites_cov, "sites were covered at least once. This is", percent_cov, "% of the reference genome")

	# Let's plot shit - not totally necessary but interesting

	png(paste0(basedir,"/sample_lists/presence.png"),  width = 2000, height = 1500, units = "px")
	ggplot(presence_hist) +
	  geom_point(aes(x=by, y=n)) +
	  theme_cowplot()
	dev.off()

	png(paste0(basedir,"/sample_lists/depth.png"),  width = 2000, height = 1500, units = "px")
	ggplot(depth_hist) +
	  geom_point(aes(x=by, y=n)) +
	  theme_cowplot()
	dev.off()

	png(paste0(basedir,"/sample_lists/bases_vs_prop_ref_covered.png"),  width = 2000, height = 1500, units = "px")
	ggplot(per_position_per_ind) +
	  geom_point(aes(x=mean_depth, y=proportion_of_reference_covered)) +
	  theme_cowplot()
	dev.off()

	## Design filters for SNP calling

	## Super low and super high coverage sites are cut from this figure
	png(paste0(basedir,"/sample_lists/super_low_and_super_high_cut1.png"),  width = 2000, height = 1500, units = "px")

	filter(depth_hist, by>10, by<700) %>%
	  ggplot(aes(x=by, y=n)) +
	  geom_freqpoly(stat = "identity") +
	  theme_cowplot()

	## 176 is the mode of the second peak
	filter(depth_hist, by>10) %>%
	  arrange(by=n) %>%
	  tail(n=1)

	filter(depth_hist, by<345) %>% #176# by<x, where x = mode of the second peak
	  arrange(by=n) %>%
	  head(n=1)

	mode_line <- 345
	max_depth <- 590 #345-100+345 #350 # try and get this so that it makes the depth a normal distribution
	cat'If MaxDepth=', max_depth, ' filter is used, ', round(sum(filter(depth_hist, by>max_depth)$n)/sum(depth_hist$n)*100,2), '% of all sites and ', round(sum(filter(depth_hist, by>max_depth)$n*filter(depth_hist, by>max_depth)$by)/sum(depth_hist$n*depth_hist$by)*100,2), '% of the final mapped data will be lost. ')

	min_depth <- 100#39 # Set this to be the first trough
	cat('If minDepth=', min_depth, ' filter is used, ', round(sum(filter(depth_hist, by<min_depth)$n)/sum(depth_hist$n)*100,2), '% of all sites and ', round(sum(filter(depth_hist, by<min_depth)$n*filter(depth_hist, by<min_depth)$by)/sum(depth_hist$n*depth_hist$by*100,2)), '% of the final mapped data will be lost. ')

	## If these filters are used
	filter(depth_hist, by>1, by<700) %>%
	ggplot(aes(x=by, y=n)) +
	  geom_freqpoly(stat = "identity") +
	  geom_vline(xintercept = c(max_depth,min_depth), color="red") +
	  theme_cowplot()

	dev.off()
	
In the slurm `.out` file it tells me (thanks to the code above):

	"The mean depth per position across all individuals is 282.68 and the standard deviation is 160.5406. A total of 867185962 sites were covered at least once. This is 95.22 % of the reference genome."
	
	"If MaxDepth= 590  filter is used,  0.89 % of all sites and  2.05 % of the final mapped data will be lost. If minDepth= 100  filter is used,  19.32 % of all sites and  0 % of the final mapped data will be lost."
	
![Alt text](https://github.com/mayroberts/lcwgs-population-analysis-minus-the-cursing/blob/c897f9446a6ec307534729561cc79e508bec9542/super_low_and_super_high_cut1.png)

