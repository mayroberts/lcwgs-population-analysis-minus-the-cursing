# Stage 1: Data pre-processing
Low coverage whole genome sequencing analysis journey...including a lot of the all the nitty gritty bits 'cause we're learnin'\
All work run through slurm/university cluster\
This is version # 2.6k

- [Data Info](#data-info)
- [Data pre-process](#data-pre-process)
	- [Moving data](#Gather-sequences)
	- [Making support files of sample tables and sample lists for later use](#Create-sample-table-and-sample-lists)
	- [Fastp - QC, adapter trimming](#Fastp-QC-and-adapter-trimming)
	- [Map, filter, reindex, reads](#Map-and-filter-reads)
	- [Merge replicate libraries if you got 'em](#Merge-replicate-libraries-of-samples)
	- [Deduplicate and clip overlapping portions of reads and reindex](#Deduplicate-and-clip-overlapping-portions-of-reads)

# Data Info
Low coverage data of 120+ samples of fish in 12 populations around the Pacific\
Libraries were prep'ed using Tn5 transposase (prob wouldn't rec at this point)\
Pools of Tn5 libraries named MR01, MR02, MR03, MR05, MR06, MR07, sent to Novogene in Sacramento, CA, USA

Data is stored in 3 directories: \
/hb/groups/bernardi_lab/may/data/pop_data/lcwgs/ \
WGS_09102021_NovogeneF001 libraries: (MR01,MR02) \
WGS_09102021_NovogeneF002 library: (MR03) \
WGS_05082022_NovogeneF001 libraries: (MR05, MR06, MR07)

# Data pre-process
## Get data in the right place and make tables and lists to be used throughout
### Gather sequences
To process, I copy all the `fq.gz` files in each of their individual directories to a directory called `all_raw_seq_data` using `gather_code.txt` which will look for and `find` all files within directories that end in `.fq.gz`and copy them to the destination file `GATHERED_DIR`.\
I run this from the directory that contains all the individual directories that hold the sequence files that I want (ie `WGS_05082022_NovogeneF001/raw_data/`. I did this directly in the command line (copy and paste lines below) on the head node.\

`gather_code.txt`

    DATA_DIR=/hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/pools01-03/WGS_09102021_NovogeneF001/raw_data
    GATHERED_DIR=/hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/pools01-03/all_raw_seq_data
   
    cd $DATA_DIR
    find . -type f -name *.fq.gz -exec cp {} $GATHERED_DIR \;

### Check MD5 to make sure data transfer was complete

    ls *.gz > list.txt
    list=./list.txt
    head list.txt
    for file in `cat $list`; do md5sum $file >> MD5check.txt; done
    
Here, I just copy and paste the lists of MD5s from the file of pre-transfer MD5s from the sequenceing facility and the list of MD5s produced here and paste into excel where I check for duplicate cells function (Home>Conditional Formatting>Highlight Cell Rules>Duplicate Values). Hopefully all the MD5s are duplicated showing that the file tranfers were successfully completed. 

## Create sample table and sample lists
Adapted from: Physalia Day 1 tutorial: https://github.com/nt246/physalia-lcwgs/blob/main/day_1/markdowns/data_processing.md#4-make-sure-youre-familiar-with-for-loops-and-how-to-assign-and-call-variables-in-bash

For the scripts below to work, the sample table has to be a tab deliminated table with the following six columns, strictly in this order:

1. prefix: the raw fastq file names **without** the `_1.fq.gz` ** ie. one file name per pair of reads

2. lane_number lane number; each sequencing lane or batch should be assigned a unique identifier. This is important so that if you sequence a library across multiple different sequencing lanes, you can keep track of which lane/batch a particular set of reads came from (important for accounting for sequencing error patterns or batch effects).

3. seq_id sequence ID; this variable is only relevant when different libraries were prepared out of the same sample and were run in the same lane (e.g. if you wanted to include a replicate). In this case, seq_id should be used to distinguish these separate libraries. If you only have a single library prepared from each of your samples (even if you sequence that same library across multiple lanes), you can just put 1 for all samples in this column.

4. sample_id sample ID; a unique identifier for each individual sequenced

5. population population name; the population or other relevant grouping variable that the individual belongs to

6. data_type data type; there are only two allowed entries here: pe (for paired-end data) or se (for single end data). We need this in the table because for some of our processing steps, the commands are slightly different for paired-end and single-end data.

7. *I added a column of 'uniq_ids' which is essentially: col4'_'col5'_'col3'_'col2'_' - it just simplified the scripts down the line for me.

It is important to make sure that the combination of sample_id, seq_id, and lane_number is unique for each fastq file. *ie. uniq_id

We need a second file that we call fastq list. This is simply a list of prefixes for the samples we want to analyze. Our sample table can contain data for all individuals in our study, but at any given time, we may only want to perform an operation on a subset of them. 

To make the table, first I wanted to see what all the headers were for each file. To make a list of file names and their headers:\
`get_headers.sh`

        cd /hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/new_raw_data
        DIR=/hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/sample_lists
        OUTFILE=file_headers2.txt
        CPDIR=/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists

        for i in ./*.fq.gz;
        do
                echo $(echo $i; zcat $i| head -n 1) >> $DIR/$OUTFILE
        done

Then I pasted the info into excel to make the table and list because, call me a *dingdong*, but I like seeing WTF is going on. \
How I chose what info went into the columns for the table:
`sample_table.tsv`\
1. prefix: The whole, messy, file name without the fq.gz file ending
2. lane #: The number after L - caveat: for the most recent sequencing run, in order to make sure that those reads would be differentiated from the first batch, I changed lane 1 to 3 and 2 to 4. 
3. seq_ID: the 7th or 8th letter of the file name. If it is a number it is a replicate sequence (differently indexed individual), if it is a "C" there is no replicate. - Need to check whether I am resequencing some individuals. - I am IRI16!
4. sample_ID: First 5 letters of the file name \ ex:AMA04 \
5. population_ID: First 3 letters of the file name \ ex: AMA\
6. data type: they are all paired end reads so "pe" for all \
`sample_list`

## Fastp-QC and adapter trimming
https://github.com/OpenGene/fastp \
Runtime : 9+ hours \
Can't remember if I tried making this an array - would be good here though if possible.\

`1-fastp.mpi`

        #!/bin/bash
        #SBATCH --job-name fp_all
        #SBATCH --output=%j-fastp_all.out
        #SBATCH --error=%j-fastp_all.err
        #SBATCH --mail-type=ALL
        #SBATCH --mail-user=mabrober@ucsc.edu
        #SBATCH --partition=128x24
        #SBATCH --nodes=1
        #SBATCH --time=30-00:00:00
        #SBATCH --mem=120GB
        #SBATCH --ntasks-per-node=16

        module load miniconda3.9
        conda activate /hb/scratch/mabrober/programs/fastp

        #Hybrid code with physalia (which uses fastqc - I use fastp) to mostly to have naming of downstream files be consistent.

        RAWDATA=/hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/all_raw_data #path to raw fq.gz files
        BASEDIR=/hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/
	OUTDIR=reports_fastp ## rename this if you're running again it will make a empty output directory 
        SAMPLELIST=/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files. It can be a subset of the the 1st column of the sample table.
        SAMPLETABLE=/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists/sample_table_all.tsv # Path to a sample table where the 1st column is the prefix of the raw fastq files. The 4th column is the sample ID, the 2nd column is the lane number, and the 3rd column is sequence ID. The combination of these three columns have to be unique. The 6th column should be data type, which is either pe or se.
        RAW_R1=_1.fq.gz # Suffix to raw fastq files. Use forward reads with paired-end data.
        RAW_R2=_2.fq.gz # Suffix to raw fastq files. Use reverse reads with paired-end data.
        HTML=$BASEDIR/$OUTDIR/html_reports #where output reports will live
        JSON=$BASEDIR/$OUTDIR/json_reports
        
	## Make output directories ## rename these if you're running again it will make a empty output directory 
	mkdir $$BASEDIR/$OUTDIR
	mkdir $HTML
	mkdir $JSON
	
        ## Loop over each sample

        for SAMPLEFILE in `cat $SAMPLELIST`; do

        ## Extract relevant values from a table of sample, sequencing, and lane ID (here in columns 4, 3, 2, respectively) for each sequenced library. This is for the naming of trimmed/processed files
            SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4`
            POP_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 5`
            SEQ_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 3`
            LANE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 2`
            SAMPLE_UNIQ_ID=$SAMPLE_ID'_'$POP_ID'_'$SEQ_ID'_'$LANE_ID  # When a sample has been sequenced in multiple lanes, we need to be able to identify the files from each run uniquely

        ## The input and output path and file prefix
                SAMPLEADAPT=$OUTDIR'/adapter_clipped/'$SAMPLE_UNIQ_ID

               fastp -i $RAWDATA/$SAMPLEFILE$RAW_R1\
                -I $RAWDATA/$SAMPLEFILE$RAW_R2\
                -o ${SAMPLEADAPT}_adapter_clipped_f_paired.fastq.gz\
                -O ${SAMPLEADAPT}_adapter_clipped_r_paired.fastq.gz\
                -l 30\
                -h $HTML/${SAMPLE_UNIQ_ID}_fastp.html \
                -j $JSON/${SAMPLE_UNIQ_ID}_fastp.json 
        done
	
### Mulitqc - can't say enough beautiful things about this program       
Once `fastp` is done, go to the `reports_html` directory where all the individual .html files were output to (/hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/reports_fastp). Then,

    module load miniconda3.9
    conda activate multiqc
    multiqc .
    
This will pull together and summarize all the reports into one pretty html report. <3

## Map and filter reads
Bowtie2 http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#the-bowtie2-build-indexer\
### Index reference bowtie2-build
Before we map the reads to the reference genome, we index them
runtime:17min\
`1-bowtie_index.mpi`

        #!/bin/bash
        #
        #SBATCH --job-name bt-index
        #SBATCH --output=%j-bt-index.out
        #SBATCH --error=%j-bt-index.err
        #SBATCH --mail-type=ALL
        #SBATCH --mail-user=mabrober@ucsc.edu
        #SBATCH --partition=128x24
        #SBATCH --nodes=1
        #SBATCH --time=30-00:00:00
        #SBATCH --mem=120GB
        #SBATCH --ntasks-per-node=12

        module load miniconda3.9
        conda activate bowtie2
        
        DIR=/hb/groups/bernardi_lab/may/DTR/DTRgenome/assembly/assembly-versions/masurca-wkfl/5-final-hic-mappped/
        REF=$DIR/kuro_gaps_homogenized_output.fasta

        cd $DIR

        bowtie2-build $REF ./

### Map and filter - bowtie2
Using an array since I have 190 pe seqs to process.\
Run tim: 7 hours 
`2-mapnfilt_array.mpi`

        #!/bin/bash
        #SBATCH --job-name mapnfilt
        #SBATCH --output=%A_%a_mapnfilt.out
        #SBATCH --error=%A_%a_mapnfilt.err
        #SBATCH --mail-type=ALL
        #SBATCH --mail-user=mabrober@ucsc.edu
        #SBATCH --partition=128x24
        #SBATCH --time=30-00:00:00
        #SBATCH --mem=5GB
        #SBATCH --array=1-139%24 #need this to tell slurm I want an array with 139 iterations(samples) with up to 24 running at a time (calc available mem and threads)
        #SBATCH --ntasks=1

        echo job: mapping (bowtie) and filtering (samtools)
        module load miniconda3.9
        conda activate bowtie2
        module load samtools

    ## Where is everything, what are they called
        BASEDIR=/hb/groups/bernardi_lab/may/DTR/population-analysis/
        OUTDIR=$BASEDIR/1-mapnfilter/bams/
        SAMPLELIST=$BASEDIR/sample_lists/sample_list.txt # Path to a list of prefixes of the raw fastq files (looks like "PHI06_3_CKDL210014824-1a-AK13786-AK40299_HJM3WDSX2_L2"). Subset of 1st column ofsample table.
        SAMPLETABLE=$BASEDIR/sample_lists/sample_table_all_uid.tsv # Path to a sample table of info for each sample seqs
        FASTQDIR=/hb/groups/bernardi_lab/may/data/pop_data/lcWGS_data/adapter_clipped/ # Path to the directory of input fastq files
        FASTQSUFFIX1=_adapter_clipped_f_paired.fastq.gz # Suffix to forward fastq files
        FASTQSUFFIX2=_adapter_clipped_r_paired.fastq.gz # Suffix to reverse fastq files
        REF=/hb/groups/bernardi_lab/may/DTR/DTRgenome/assembly/assembly-versions/masurca-wkfl/5-final-hic-mappped/kuro_gaps_homogenized_output.fasta # Path to reference fasta file and file name
        REFNAME=kuro # Reference genome name to add to output files

    ## Set parameter
        MAPPINGPRESET=very-sensitive # The pre-set option to use for mapping in bowtie2 (very-sensitive for end-to-end (global) mapping [typically used when we have a full genome reference], very-sensitive-local for partial read mapping that allows soft-clipping [typically used when mapping genomic reads to a transcriptome]

    ## Make sure the array creates jobs for each line in sample_list.txt
        SAMPLEFILE=$(sed -n "${SLURM_ARRAY_TASK_ID},1p" $SAMPLELIST)
        echo Slurm array task ID is: $SLURM_ARRAY_TASK_ID
        echo Samplefile is $SAMPLEFILE

    ## Extract relevant values from a table of sample, sequencing, and lane ID (here in columns 4, 3, 2, respectively) for each sequenced library
        SAMPLE_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 4`
        POP_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 5`
        SEQ_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 3`
        SAMPLE_UNIQ_ID=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 7 `
        PU=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 2`
        DATATYPE=`grep -P "${SAMPLEFILE}\t" $SAMPLETABLE | cut -f 6`

    ## The input and output path and file prefix
        SAMPLETOMAP=$FASTQDIR$SAMPLE_UNIQ_ID
        SAMPLEBAM=$OUTDIR$SAMPLE_UNIQ_ID

    ## Define reference base name
        REFBASENAME="${REF%.*}"

    # Map reads to the reference
        echo sample uniq ID is $SAMPLE_UNIQ_ID

        bowtie2 -q --phred33 --$MAPPINGPRESET -p 1 -I 0 -X 1500 --fr --rg-id $SAMPLE_UNIQ_ID --rg SM:$SAMPLE_ID --rg LB:$SAMPLE_ID --rg PU:$PU --rg PL:ILLUMINA -x $REFBASENAME -1 $SAMPLETOMAP$FASTQSUFFIX1 -2 $SAMPLETOMAP$FASTQSUFFIX2 -S $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.sam'

        ## Convert to bam file for storage (including all the mapped reads)
        samtools view -bS -F 4 $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.sam' > $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.bam'
        rm -f $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.sam'

        ## Filter the mapped reads (to only retain reads with high mapping quality)
        # Filter bam files to remove poorly mapped reads (non-unique mappings and mappings with a quality score < 20) -- do we want the quality score filter??
        samtools view -h -q 20 $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'.bam' | samtools view -buS - | samtools sort -o $SAMPLEBAM'_'$DATATYPE'_bt2_'$REFNAME'_minq20_sorted.bam'
    echo Weeeeeeeeee!
    
### 
'3-checkoutput.mpi'\


## Merge replicate libraries of samples
Run time : ~2 hours to merge over ~110 files into ~30\
`4-merge_reps.mpi`

        #!/bin/bash
        #
        #SBATCH --job-name merge-reps
        #SBATCH --output=%j-merge_reps.out
        #SBATCH --error=%j-merge_reps.err
        #SBATCH --mail-type=ALL
        #SBATCH --mail-user=mabrober@ucsc.edu
        #SBATCH --partition=128x24
        #SBATCH --nodes=1
        #SBATCH --time=30-00:00:00
        #SBATCH --mem=120GB
        #SBATCH --ntasks-per-node=6

        module load samtools

        cd /hb/groups/bernardi_lab/may/DTR/population-analysis/1-mapnfilter/bams/
        OUTDIR=/hb/groups/bernardi_lab/may/DTR/population-analysis/1-mapnfilter/merge_test/
        #file=("BAK12_BAK" "BAK13_BAK" "FAL02_FAL" "FAL04_FAL" "FAL05_FAL" "FAL06_FAL" "FAL07_FAL""IRI07_IRI" "IRI08_IRI" "IRI09_IRI" "IRI13_IRI" "IRI16_IRI" "KIN05_KIN" "KIN08_KIN" "KIN06_KIN" "MOO01_MOO" "MOO04_MOO" "MOO05_MOO" "MOO06_MOO" "MOO07_MOO" "OOUR01_OKI" "OOUR01_OKI" "OZPA03_OKI" "OZPA04_OKI" "PHI05_PHI" "PHI06_PHI" "PHI07_PHI") #list file name groupings here #must do "OZPA07_OKI" still
        #file=("FAL07_FAL") #fix
        file=("MOO07_MOO")  #another fix
	
        for i in ${file[@]};   #for all items in $file
        do
        input_file_1="${i}_1_*_pe_bt2_kuro_minq20_sorted.bam"
        input_file_2="${i}_2_*_pe_bt2_kuro_minq20_sorted.bam"
        #input_file_3="${i}_3_*_pe_bt2_kuro_minq20_sorted.bam"
        out_file="${i}_M_2_pe_bt2_kuro_minq20_sorted.bam" #M for merged # luckily, all file to be merged were in lane 2

        samtools merge $OUTDIR/$out_file $input_file_1 $input_file_2 #$input_file_3
        done

## Deduplicate and clip overlapping portions of reads
Runtime: damn, deleted the email with the info I guess\
'5-dedup_clip.mpi'

        #!/bin/bash
        #
        #SBATCH --job-name dedup_clip
        #SBATCH--output=%A_%a_dedup_clip.out
        #SBATCH --error=%A_%a_dedup_clip.err
        #SBATCH --mail-type=ALL
        #SBATCH --mail-user=mabrober@ucsc.edu
        #SBATCH --partition=128x24
        ##SBATCH --nodes=1
        #SBATCH --time=30-00:00:00
        #SBATCH --array=1-2%
        #SBATCH --mem=90GB
        #SBATCH --ntasks-per-node=24
        echo `date` job: removing duplicate reads (picard) and clipping overlapping ends of the f/r paired reads (bamutil)

        module load miniconda3.9
        conda activate /hb/scratch/mabrober/programs/arima_programs/picard

        BAMDIR=/hb/groups/bernardi_lab/may/DTR/population-analysis/1-mapnfilter/bams
        BAMLIST=/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists/dedup_fixlist2.txt
        REF=/hb/groups/bernardi_lab/may/DTR/DTRgenome/assembly/assembly-versions/masurca-wkfl/5-final-hic-mappped//kuro_gaps_homogenized_output.fasta
        REFNAME=kuro # Reference name to add to output files
        PICARD=/hb/scratch/mabrober/programs/arima_programs/picard/share/picard-2.26.2-0/picard.jar
        BAMUTIL=/hb/home/mabrober/.conda/envs/bamutil/bin/bam #bam -h for usage

        SAMPLEBAM=$(sed -n "${SLURM_ARRAY_TASK_ID},1p" $BAMLIST)
        echo Slurm array task ID is: $SLURM_ARRAY_TASK_ID
        echo Sample bam is $SAMPLEBAM'pe_bt2_kuro_minq20_sorted.bam'

        ## Loop over each sample
        #for SAMPLEBAM in `cat $BAMLIST`; do

        ## Remove duplicates and print dupstat file
            java -Xmx60g -jar $PICARD MarkDuplicates I=$BAMDIR/$SAMPLEBAM'pe_bt2_kuro_minq20_sorted.bam' O=$BAMDIR/$SAMPLEBAM'bt2_kuro_minq20_sorted_dedup.bam' M=$BAMDIR/$SAMPLEBAM'bt2_kuro_minq20_sorted_dupstat.txt' VALIDATION_STRINGENCY=SILENT REMOVE_DUPLICATES=true

        ## Clip overlapping paired end reads (only necessary for paired-end data)
            conda activate bamutil
            bam clipOverlap --in $BAMDIR/$SAMPLEBAM'bt2_kuro_minq20_sorted_dedup.bam' --out $BAMDIR/deduped_overlap_clipped_bams/$SAMPLEBAM'bt2_kuro_minq20_sorted_dedup_overlapclipped.bam' --stats

        echo done-zo woot! 
        
### reindex the deduplicated and overlap - clipped reads 
 - while also making a new list for next step\
Runtime: 20 min?
 '6-reindex.mpi"
 
        #!/bin/bash
        #
        #SBATCH --job-name index
        #SBATCH--output=%A_%a-index.out
        #SBATCH --error=%A_%a-index.err
        #SBATCH --mail-type=ALL
        #SBATCH --mail-user=mabrober@ucsc.edu
        #SBATCH --partition=128x24
        #SBATCH --nodes=1
        #SBATCH --time=30-00:00:00
        #SBATCH --mem=20GB
        #SBATCH --array=1-138%18
        #SBATCH --ntasks-per-node=4

        echo `date` job: re-index bam files

        BAMDIR=/hb/groups/bernardi_lab/may/DTR/population-analysis/1-mapnfilter/bams/deduped_overlap_clipped_bams/
        BAMLIST=/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists/index_bam_list_dedup_overlapclipped_fix.list
        REF=/hb/groups/bernardi_lab/may/DTR/DTRgenome/assembly/assembly-versions/masurca-wkfl/5-final-hic-mappped/kuro_gaps_homogenized_output.fasta
        REFNAME=kuro # Reference name to add to output files

        #Set up array - assigns sample to individula job within array from BAMLIST
        SAMPLEBAM=$(sed -n "${SLURM_ARRAY_TASK_ID},1p" $BAMLIST)
        echo Slurm array task ID is: $SLURM_ARRAY_TASK_ID
        echo Sample bam is $SAMPLEBAM'bt2_kuro_minq20_sorted_dedup_overlapclipped.bam'

        module load samtools

        cd $BAMDIR/
        samtools index $SAMPLEBAM'bt2_kuro_minq20_sorted_dedup_overlapclipped.bam'
      
### cursory check bams
Here, I don't know if this is legit but I just `ls -lah` the dir with all the $SAMPLEBAMbt2_kuro_minq20_sorted_dedup_overlapclipped.bams and `mv`ed any bams that were <500M in size into a`bad_bams` dir. \_"]_/

### make list 
Now we need lists of bams preceded by paths to the bam files.  
 
This one below makes a list of ALL the bams. To make population level lists you could use `grep "AMA" > AMA.list` for example, to make the population lists. 
 
`make.list.sh`
        LISTDIR=/hb/groups/bernardi_lab/may/DTR/population-analysis/sample_lists
        BAMDIR=/hb/groups/bernardi_lab/may/DTR/population-analysis/bams/deduped_overlap_clipped_bams
	LIST=bam.list
	
        for SAMPLEBAM in `cat $LISTDIR/$LIST`; do
                echo $BAMDIR/$SAMPLEBAM'bt2_kuro_minq20_sorted_dedup_overlapclipped.bam' >> $LISTDIR/bam_path_list.txt
        done
       
## Move on to 2.0 read counts, summarizing data, and snp filter paramters       
